{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "791f0e12-bbb6-4bcb-8a2e-1048ab03bbd2",
   "metadata": {},
   "source": [
    "# Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a418dd8-90ca-41b5-b4ea-9638598d471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d116ad26-12a3-487c-9b47-5f9abe2b6a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copom = pd.read_csv('df_copom_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60443267-16fc-4610-9633-d96d57c5a6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Selic</th>\n",
       "      <th>Meeting_Number</th>\n",
       "      <th>Decision</th>\n",
       "      <th>Decision_txt</th>\n",
       "      <th>label_hawk_dove</th>\n",
       "      <th>label_next_meet</th>\n",
       "      <th>Text</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006/03/08</td>\n",
       "      <td>16.50</td>\n",
       "      <td>117.0</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>decrease</td>\n",
       "      <td>dovish</td>\n",
       "      <td>decrease</td>\n",
       "      <td>In the March Meeting, the Banco Central do Br...</td>\n",
       "      <td>Statement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006/04/19</td>\n",
       "      <td>15.75</td>\n",
       "      <td>118.0</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>decrease</td>\n",
       "      <td>dovish</td>\n",
       "      <td>decrease</td>\n",
       "      <td>In the April Meeting, the Monetary Policy Com...</td>\n",
       "      <td>Statement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006/05/31</td>\n",
       "      <td>15.25</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>decrease</td>\n",
       "      <td>dovish</td>\n",
       "      <td>decrease</td>\n",
       "      <td>In the May Meeting, the Monetary Policy Commi...</td>\n",
       "      <td>Statement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006/07/19</td>\n",
       "      <td>14.75</td>\n",
       "      <td>120.0</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>decrease</td>\n",
       "      <td>dovish</td>\n",
       "      <td>decrease</td>\n",
       "      <td>In the July Meeting, the Copom unanimously de...</td>\n",
       "      <td>Statement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006/08/30</td>\n",
       "      <td>14.25</td>\n",
       "      <td>121.0</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>decrease</td>\n",
       "      <td>dovish</td>\n",
       "      <td>decrease</td>\n",
       "      <td>In the August Meeting, the Copom unanimously ...</td>\n",
       "      <td>Statement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Selic  Meeting_Number  Decision Decision_txt label_hawk_dove  \\\n",
       "0  2006/03/08  16.50           117.0     -0.75     decrease          dovish   \n",
       "1  2006/04/19  15.75           118.0     -0.75     decrease          dovish   \n",
       "2  2006/05/31  15.25           119.0     -0.50     decrease          dovish   \n",
       "3  2006/07/19  14.75           120.0     -0.50     decrease          dovish   \n",
       "4  2006/08/30  14.25           121.0     -0.50     decrease          dovish   \n",
       "\n",
       "  label_next_meet                                               Text  \\\n",
       "0        decrease   In the March Meeting, the Banco Central do Br...   \n",
       "1        decrease   In the April Meeting, the Monetary Policy Com...   \n",
       "2        decrease   In the May Meeting, the Monetary Policy Commi...   \n",
       "3        decrease   In the July Meeting, the Copom unanimously de...   \n",
       "4        decrease   In the August Meeting, the Copom unanimously ...   \n",
       "\n",
       "        Type  \n",
       "0  Statement  \n",
       "1  Statement  \n",
       "2  Statement  \n",
       "3  Statement  \n",
       "4  Statement  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copom.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48226f65-b901-44e2-b066-bdc45a9dda50",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df_copom['Text'].tolist()\n",
    "labels = df_copom['label_hawk_dove'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aa46a1e-95fb-4ddb-8fe2-7a8ad5383a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Create a tokenizer object\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit the tokenizer on the text data\n",
    "tokenizer.fit_on_texts(texts)\n",
    "tokenizer.fit_on_texts(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8a8b86c-9391-4baf-9aff-59221bb3faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec0e4ec8-db5f-482f-b874-61d992407589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ftrav\\anaconda3\\envs\\pytorchenv\\lib\\site-packages\\ipykernel_launcher.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\ftrav\\anaconda3\\envs\\pytorchenv\\lib\\site-packages\\ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the text and convert to sequences (using the same tokenizer as before)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "sequences_train = [torch.tensor(seq) for seq in sequences_train]\n",
    "sequences_test = [torch.tensor(seq) for seq in sequences_test]\n",
    "\n",
    "# Pad sequences to have the same length\n",
    "padded_sequences_train = pad_sequence(sequences_train, batch_first=True)\n",
    "padded_sequences_test = pad_sequence(sequences_test, batch_first=True)\n",
    "\n",
    "# Convert the padded sequences back to numpy arrays if needed\n",
    "padded_sequences_train = padded_sequences_train.numpy()\n",
    "padded_sequences_test = padded_sequences_test.numpy()\n",
    "\n",
    "# Convert labels to one-hot encoded tensors\n",
    "label_classes = list(set(labels))\n",
    "num_classes = len(label_classes)\n",
    "\n",
    "label_to_index = {label: index for index, label in enumerate(label_classes)}\n",
    "index_to_label = {index: label for label, index in label_to_index.items()}\n",
    "\n",
    "labels_encoded_train = np.array([label_to_index[label] for label in y_train])\n",
    "labels_encoded_test = np.array([label_to_index[label] for label in y_test])\n",
    "\n",
    "labels_one_hot_train = torch.eye(num_classes)[labels_encoded_train]\n",
    "labels_one_hot_test = torch.eye(num_classes)[labels_encoded_test]\n",
    "\n",
    "# Convert data to PyTorch tensors and create DataLoader for training and testing datasets\n",
    "X_train_tensor = torch.from_numpy(padded_sequences_train).long()\n",
    "y_train_tensor = torch.tensor(labels_one_hot_train).float()\n",
    "\n",
    "X_test_tensor = torch.from_numpy(padded_sequences_test).long()\n",
    "y_test_tensor = torch.tensor(labels_one_hot_test).float()\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "460c32d3-3100-45fe-896c-7d844832a3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        output = self.fc(hidden.squeeze(0))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10c1d38c-f98b-4969-a8d5-528321044c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 100\n",
    "hidden_dim = 100\n",
    "output_dim = num_classes\n",
    "\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b70160-dd5a-4213-8533-c403485a5116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Test Accuracy: 0.375\n",
      "Epoch [2/10], Test Accuracy: 0.40625\n",
      "Epoch [3/10], Test Accuracy: 0.21875\n",
      "Epoch [4/10], Test Accuracy: 0.21875\n",
      "Epoch [5/10], Test Accuracy: 0.21875\n",
      "Epoch [6/10], Test Accuracy: 0.21875\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.argmax(dim=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            predicted = outputs.argmax(dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.argmax(dim=1)).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1e0867-cd17-4a02-8a26-b04ee6311815",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
