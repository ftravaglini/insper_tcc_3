{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfd69430-df78-4d27-aa97-f15c8553a42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63bee8b2-a544-4818-bbe6-a93f1b838571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d5862a3-6a93-4315-b021-768bb711e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gtfintechlab/FOMC-RoBERTa\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"gtfintechlab/FOMC-RoBERTa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78756fdb-eeec-4e2e-8b80-4db2c366738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10cbf78f-050d-48f1-beb8-1ce1e4b1e023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4a36e9d-1942-4f15-9afd-e7acbe9d5b47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'emotions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m emotions_encoded \u001b[38;5;241m=\u001b[39m \u001b[43memotions\u001b[49m\u001b[38;5;241m.\u001b[39mmap(tokenize, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'emotions' is not defined"
     ]
    }
   ],
   "source": [
    "emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b810611-f80a-4dc4-8572-9b57cee5580f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "621f1afa-6cc8-461f-8063-3c14cd43807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e33cb97-8647-4f61-a31d-a2b226428c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = (AutoModelForSequenceClassification\n",
    "        .from_pretrained(model_ckpt, num_labels=num_labels)\\\n",
    "        .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bff2097a-95ac-4669-b68f-055e0aa8761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f81c0ba-c636-4829-b4f4-ce724d4d0a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d48b2ca6-4198-44d2-9268-86cac8cdaf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "599505f0-55a5-439b-bb80-c5e04a8fdfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a972786c-b4a9-4fcc-b3bf-4c88ba614148",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14765595-a722-48a0-ad0f-152a235f2f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_encoded.set_format('torch',\n",
    "                           columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24e2aba4-7dc3-4f27-b041-016713256f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "logging_steps = len(emotions_encoded['train']) // batch_size\n",
    "model_name = f'{model_ckpt}-finetuned-emotion'\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                 num_train_epochs=2,\n",
    "                                 learning_rate=2e-5,\n",
    "                                 per_device_train_batch_size=batch_size,\n",
    "                                 per_gpu_eval_batch_size=batch_size,\n",
    "                                 weight_decay=0.01,\n",
    "                                 evaluation_strategy='epoch',\n",
    "                                 disable_tqdm=False,\n",
    "                                 logging_steps=logging_steps,\n",
    "                                 push_to_hub=False,\n",
    "                                 log_level='error'\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea4a963c-f604-4270-936f-6e8774a8dd80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emotions_encoded['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f08d9a1a-595d-4532-9a1b-412100ed7b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a7b673e-d12f-4202-9623-13710ad8ec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=emotions_encoded['train'],\n",
    "                  eval_dataset=emotions_encoded['validation'],\n",
    "                  tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6aaa6818-b8c9-4d17-8962-2f17e65d4ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ftrav\\anaconda3\\envs\\pytorchenv\\lib\\site-packages\\transformers\\optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 19:00, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.823800</td>\n",
       "      <td>0.322954</td>\n",
       "      <td>0.903000</td>\n",
       "      <td>0.900274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.248000</td>\n",
       "      <td>0.216912</td>\n",
       "      <td>0.927000</td>\n",
       "      <td>0.927135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.5358845138549805, metrics={'train_runtime': 1142.8519, 'train_samples_per_second': 28.0, 'train_steps_per_second': 0.438, 'total_flos': 720342861696000.0, 'train_loss': 0.5358845138549805, 'epoch': 2.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c3d8bed-50fd-454c-990f-3b765cbf2b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds_output = trainer.predict(emotions_encoded['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "471bccd6-017b-4cd1-b009-1ddc8e9e2710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.21691159904003143,\n",
       " 'test_accuracy': 0.927,\n",
       " 'test_f1': 0.9271345963162837,\n",
       " 'test_runtime': 8.167,\n",
       " 'test_samples_per_second': 244.888,\n",
       " 'test_steps_per_second': 3.918}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_output.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17083edb-8208-41f9-be20-514c9e5b4e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = np.argmax(preds_output.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f12f276b-75ad-4e9a-9481-79ca2c7f5a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5fb9bcf-4179-47e8-83a2-03e352ad9298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.927, 'f1': 0.9271345963162837}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(preds_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "470130db-3dbb-470f-8543-e7b4d102189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "# plot_confusion_matrix(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f435c9a8-4c0a-4941-9f55-7f1e3d4f6108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "def forward_pass_with_label(batch):\n",
    "    # Place all input tensors on the same device as the model\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() \n",
    "              if k in tokenizer.model_input_names}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        pred_label = torch.argmax(output.logits, axis=-1)\n",
    "        loss = cross_entropy(output.logits, batch[\"label\"].to(device), \n",
    "                             reduction=\"none\")\n",
    "\n",
    "    # Place outputs on CPU for compatibility with other dataset columns   \n",
    "    return {\"loss\": loss.cpu().numpy(), \n",
    "            \"predicted_label\": pred_label.cpu().numpy()}\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efca06e4-0c07-4b7f-9406-f9857e8ecf72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert our dataset back to PyTorch tensors\n",
    "emotions_encoded.set_format(\"torch\", \n",
    "                            columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "# Compute loss values\n",
    "emotions_encoded[\"validation\"] = emotions_encoded[\"validation\"].map(\n",
    "    forward_pass_with_label, batched=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5f6bcad-3c11-4c61-a59d-ef367c13b0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_encoded.set_format(\"pandas\")\n",
    "cols = [\"text\", \"label\", \"predicted_label\", \"loss\"]\n",
    "df_test = emotions_encoded[\"validation\"][:][cols]\n",
    "# df_test[\"label\"] = df_test[\"label\"].apply(label_int2str)\n",
    "# df_test[\"predicted_label\"] = (df_test[\"predicted_label\"].apply(label_int2str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11ae2fb6-86dc-46d0-b9eb-6b5e16f87b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>i guess we would naturally feel a sense of lon...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5.528459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>i called myself pro life and voted for perry w...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.122855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>i as representative of everything thats wrong ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5.116637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>i am going to several holiday parties and i ca...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.072585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1870</th>\n",
       "      <td>i guess i feel betrayed because i admired him ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.001413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>i feel badly about reneging on my commitment t...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4.909636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>i would eventually go in to these stores but i...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4.877432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>i feel super awkward and out of place right now</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.874084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>i feel that he was being overshadowed by the s...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4.854888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>im lazy my characters fall into categories of ...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4.411380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label  \\\n",
       "1500  i guess we would naturally feel a sense of lon...      3   \n",
       "1963  i called myself pro life and voted for perry w...      1   \n",
       "1950  i as representative of everything thats wrong ...      5   \n",
       "1274  i am going to several holiday parties and i ca...      1   \n",
       "1870  i guess i feel betrayed because i admired him ...      1   \n",
       "882   i feel badly about reneging on my commitment t...      2   \n",
       "465   i would eventually go in to these stores but i...      1   \n",
       "765     i feel super awkward and out of place right now      1   \n",
       "1801  i feel that he was being overshadowed by the s...      2   \n",
       "1111  im lazy my characters fall into categories of ...      1   \n",
       "\n",
       "      predicted_label      loss  \n",
       "1500                0  5.528459  \n",
       "1963                0  5.122855  \n",
       "1950                0  5.116637  \n",
       "1274                0  5.072585  \n",
       "1870                0  5.001413  \n",
       "882                 0  4.909636  \n",
       "465                 4  4.877432  \n",
       "765                 0  4.874084  \n",
       "1801                0  4.854888  \n",
       "1111                4  4.411380  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.sort_values(\"loss\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e22755d8-7f9b-41bd-9454-1a9dd6f69fc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Impossible to guess which tokenizer to use. Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_39244\\918325058.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# # Change `transformersbook` to your Hub username\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# model_id = \"transformersbook/distilbert-base-uncased-finetuned-emotion\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"text-classification\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mmodel_ckpt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'distilbert-base-uncased'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_ckpt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorchenv\\lib\\site-packages\\transformers\\pipelines\\__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    867\u001b[0m                 \u001b[1;31m# Impossible to guess what is the right tokenizer here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m                 raise Exception(\n\u001b[1;32m--> 869\u001b[1;33m                     \u001b[1;34m\"Impossible to guess which tokenizer to use. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m                     \u001b[1;34m\"Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m                 )\n",
      "\u001b[1;31mException\u001b[0m: Impossible to guess which tokenizer to use. Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# # Change `transformersbook` to your Hub username\n",
    "# model_id = \"transformersbook/distilbert-base-uncased-finetuned-emotion\"\n",
    "classifier = pipeline(\"text-classification\", model=model)\n",
    "tokenizer="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dd942e-f039-419a-aab0-b5d25545c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tweet = \"I saw a movie today and it was really good.\"\n",
    "preds = classifier(custom_tweet, top_k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31f319a-5ea2-4fa9-8171-6cfcd6a2324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame(preds)\n",
    "preds_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65722fbe-d533-4f0c-bcdc-25f15da5e98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(preds_df['label'], 100 * preds_df[\"score\"], color='C0')\n",
    "plt.title(f'\"{custom_tweet}\"')\n",
    "plt.ylabel(\"Class probability (%)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fd0ea4-fd3c-433d-b872-0fe9c052baad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bc5976-b357-4869-b326-171497aa9b43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be3cd73-8e3f-4572-9c4f-e76d216b09c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
